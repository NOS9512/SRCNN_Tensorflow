{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odak import np\n",
    "import matplotlib.pyplot as plt\n",
    "from odak.wave import *\n",
    "from Openholo.Openholo_python.ophpy.Depthmap import *\n",
    "from odak.learn.wave import propagate_beam as propagate_beam_torch\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from loss_function import PSNR, get_single_psnr\n",
    "from generator import *\n",
    "from tqdm import tqdm\n",
    "from __functions2 import *\n",
    "import math\n",
    "from phase_optimization import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength_r = 633e-9\n",
    "wavelength_g = 532e-9\n",
    "wavelength_b = 473e-9\n",
    "\n",
    "k_r = 2*np.pi/wavelength_r\n",
    "k_g = 2*np.pi/wavelength_g\n",
    "k_b = 2*np.pi/wavelength_b\n",
    "\n",
    "pixel_size = 8.5e-6\n",
    "shape = (756,1344,1)\n",
    "\n",
    "depth1 = 0.5\n",
    "depth2 = 0.6\n",
    "depth3 = 1\n",
    "depth4 = 1.5\n",
    "depth5 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 팩터 괄호 없이 더하면 위상 홀로그램의 프린지 패턴이 없어지는 반면 numerical recon은 깔끔함.\n",
    "# 샘플링 팩터를 괄호 포함해서 더하면 위상 홀로그램의 프린지 패턴이 있는 반면에 numerical recon은 깔끔하지 못함. \n",
    "# 그냥 BL-ASM도 프린지 패턴 있음. 대신 numerical recon이 깔끔하게 나오지 않음. \n",
    "\n",
    "# 프린지 패턴이 존재하고 numerical recon도 깔끔하게 하려면 어떻게 해야할까.. ㅠ 곱해봐야하나. \n",
    "# 샘플링 팩터를 괄호 포함해서 곱하면 위상 홀로그램의 프린지 패턴이 있고 numerical recon도 깔끔함. 근데 완전 똑같이 나옴..ㅋㅋ => 딥러닝 돌려봐야할 듯?\n",
    "\n",
    "def band_limited_angular_spectrum1(field, k, distance, dx, wavelength, sampling_factor):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-(nu*sampling_factor)/2*dx, (nu*sampling_factor)/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-(nv*sampling_factor)/2*dx, (nv*sampling_factor)/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def band_limited_angular_spectrum2(field, k, distance, dx, wavelength, sampling_factor):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-(nu*sampling_factor)/2*dx, (nu*sampling_factor)/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-(nv*sampling_factor)/2*dx, (nv*sampling_factor)/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    recon = tf.math.abs(result)/tf.reduce_max(tf.math.abs(result))\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return recon\n",
    "\n",
    "def band_limited_angular_spectrum3(field, k, distance, dx, wavelength,sampling_factor):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-nu/2*dx, nu/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-nv/2*dx, nv/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu*sampling_factor)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv*sampling_factor)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return result\n",
    "\n",
    "def band_limited_angular_spectrum4(field, k, distance, dx, wavelength,sampling_factor):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-nu/2*dx, nu/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-nv/2*dx, nv/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu*sampling_factor)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv*sampling_factor)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    recon = tf.math.abs(result)/tf.reduce_max(tf.math.abs(result))\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return recon\n",
    "\n",
    "\n",
    "def band_limited_angular_spectrum5(field, k, distance, dx, wavelength):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-nu/2*dx, nu/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-nv/2*dx, nv/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return result\n",
    "\n",
    "def band_limited_angular_spectrum6(field, k, distance, dx, wavelength):\n",
    "\n",
    "    nv, nu = field.shape[0],field.shape[1] # (756,1344)\n",
    "    x = np.linspace(-nu/2*dx, nu/2*dx, nu) # x = (1344,)  => 여기서의 샘플링 팩터의 개념을 정확히 파악해야함. \n",
    "    y = np.linspace(-nv/2*dx, nv/2*dx, nv) # y = (756,)\n",
    "    X, Y = np.meshgrid(x, y) # X, Y 모두 (756, 1344)\n",
    "    Z = X**2+Y**2 # 원형 모양 \n",
    "    h = 1./(1j*wavelength*distance)*np.exp(1j*k*(distance+Z/2/distance))\n",
    "    h = np.fft.fft2(np.fft.fftshift(h))*dx**2 # 이미지 영역이 주파수 영역으로\n",
    "    flimx = np.ceil(1/(((2*distance*(1./(nu)))**2+1)**0.5*wavelength))  # 전달함수가 엘리어싱 오류를 일으키지 않는 주파수의 범위를 나타낸 것. \n",
    "    flimy = np.ceil(1/(((2*distance*(1./(nv)))**2+1)**0.5*wavelength))  \n",
    "    mask = np.zeros((nu, nv)) \n",
    "    mask = tf.cast(mask, dtype=tf.complex64) \n",
    "    mask = (np.abs(X) < flimx) & (np.abs(Y) < flimy) # 여기서 band limit하는 듯. X와 Y는 u를 뜻함. \n",
    "    mask = set_amplitude(h, mask) # phase와 amplitude로 변환 후 새로운 field인 mask로 계산.(cos,sin,1j 사용)\n",
    "    field = tf.cast(field,dtype=tf.complex64)\n",
    "    # field = field[:,:,:,0]\n",
    "    U1 = tf.signal.fft2d(tf.signal.fftshift(field)) # 이미지 영역이 주파수 영역으로\n",
    "    U2 = mask*U1\n",
    "    result = tf.signal.ifftshift(tf.signal.ifft2d(U2)) # 다시 이미지 영역으로\n",
    "    # recon = tf.math.abs(result)\n",
    "    recon = tf.math.abs(result)/tf.reduce_max(tf.math.abs(result))\n",
    "    # recon = tf.cast(recon,tf.float32)\n",
    "\n",
    "    return recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(y_true, y_pred):\n",
    "    y_true = y_true[:,:,:,0] # angular spectrum 때만\n",
    "    y_pred = band_limited_angular_spectrum(y_pred, k_r, depth2, pixel_size, wavelength_r)\n",
    "    err = y_true - y_pred\n",
    "    loss = tf.math.reduce_mean(tf.math.square(err)) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 불러올 때 custom한 손실함수도 같이 불러온다. 그 코드에서 쓴 함수 이름으로 key도 설정됨\n",
    "model = tf.keras.models.load_model('./image_Data/models/U2_11_2d_img_e100_b8_lr4d_f06_band_angular_1_final.h5',custom_objects={'recon_loss': recon_loss}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000196DD348D08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# 2d img를 입력과 출력으로. \n",
    "\n",
    "# train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Ball_article_amp_f05~06.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Ball_article_amp_f05~06.png'))\n",
    "# train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/CBNU_article_amp_f05~06_white.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/CBNU_article_amp_f05~06_white.png'))\n",
    "train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Bird_article_amp_f05~15_new_convert.png'))\n",
    "test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Bird_article_amp_f05~15_new_convert.png'))\n",
    "# train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Mugcup_article_amp_f1.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Mugcup_article_amp_f1.png'))\n",
    "# train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/4_object_f06.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/4_object_f06.png'))\n",
    "\n",
    "# train_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Ball_article_amp_f05~06.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/Ball_article_amp_f05~06.png'))\n",
    "\n",
    "# train_img = np.asarray(Image.open('./image_data/indoor_train/1.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/indoor_train/1.png'))\n",
    "\n",
    "\n",
    "# train_img = np.asarray(Image.open('./image_data/DIV2K_train_HR_512/0010.png'))\n",
    "# test_img = np.asarray(Image.open('./image_data/DIV2K_train_HR_512/0010.png'))\n",
    "\n",
    "# phase = np.expand_dims(phase[:,:,0],axis=(0,-1))\n",
    "train_img = train_img[:,:,0] # [756, 1344]\n",
    "test_img = test_img[:,:,0]\n",
    "# train_img = np.abs(train_img)/np.max(np.abs(train_img))\n",
    "# test_img = np.abs(test_img)/np.max(np.abs(test_img))\n",
    "train_img = np.expand_dims(train_img,axis=-1) # (756, 1344, 1)\n",
    "test_img = np.expand_dims(test_img,axis=-1) # (756, 1344, 1)\n",
    "train_img = np.expand_dims(train_img,axis=0) # (1, 756, 1344, 1)\n",
    "test_img = np.expand_dims(test_img,axis=0) # (1, 756, 1344, 1)\n",
    "\n",
    "\n",
    "predict = model.predict(train_img) # 훈련할거 넣기\n",
    "# predict.shape\n",
    "# plt.imshow(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = np.asarray(Image.open('./image_data/Alpha/Blender_test/object/CBNU_article_amp_f05~15_new.png'))\n",
    "# test_img = test_img[:,:,0]\n",
    "# # test_img = np.abs(test_img)/np.max(np.abs(test_img))\n",
    "# test_img = np.expand_dims(test_img,axis=-1) # (756, 1344, 1)\n",
    "# test_img = np.expand_dims(test_img,axis=0) # (1, 756, 1344, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(756, 1344) (756, 1344)\n"
     ]
    }
   ],
   "source": [
    "test_img = np.squeeze(test_img)\n",
    "predict = np.squeeze(predict)\n",
    "\n",
    "print(test_img.shape,predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict = predict.astype(np.uint8)\n",
    "plt.imshow(predict,cmap='gray') # (756,1344)\n",
    "predict\n",
    "# 왜 phase 모양이 꼭지점 4개로 분리되어서 나오는지 => cf fft한 값에 normalize한 과정이 있을수도? 중간에 normalize한 부분이 있는지 보자. normalize 과정을 없애니 나오긴 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_img/255. # normalize보다 255로 나누자\n",
    "plt.imshow(test_img,cmap='gray')\n",
    "print(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random.random((756,1344))*2*np.pi # 2 파이를 곱해야 더 괜춘\n",
    "\n",
    "complex_field1 = test_img*np.exp(1j*random)\n",
    "complex_field2 = predict*np.exp(1j*random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = np.fft.fft2(np.fft.fftshift(complex_field1))\n",
    "# frequency1 = np.abs(frequency)/np.max(np.abs(frequency))\n",
    "# plt.imshow(frequency1,cmap='gray')\n",
    "# print(frequency1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling factor 없을 때 => cgh는 강하게 나오지만 , recon에서 스펙클이 많고 정보 손실 있음.\n",
    "# sampling factor 더하기는 프린지 패턴 안 생김 (샘플링 팩터가 어떤 값이든 간에)\n",
    "# sampling factor 곱하기는 프린지 패턴 생김(0.0001,0.01) # 1.5m\n",
    "# 곱하기에서 0.0001 => 프린지 패턴, 0.01 => 새 모양 보임, 0.1 => 새 모양 보임, 1 => 샘플링 팩터 없을때와 같음 \n",
    "# , 10 => 프린지 패턴 없음.(더하기 때와 recon이 비슷. 더하기보단 조금 더 좋음), 100 => 10과 같음, 1000 => 10,100과 같음. \n",
    "\n",
    "complex_holo1 = band_limited_angular_spectrum1(complex_field1,k_r,-1.5,pixel_size,wavelength_r,0.1)\n",
    "phase = np.angle(complex_holo1)\n",
    "phase = np.abs(phase)/np.max(np.abs(phase))\n",
    "# phase = phase*np.pi\n",
    "plt.imshow(phase,cmap='gray')\n",
    "print(phase.shape)\n",
    "# plt.imsave('./image_data/slm_phase 실험해볼 것/2D_phase_band_limited_r/Bird_f15_bl_phase.png',phase,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_holo_predict = band_limited_angular_spectrum1(complex_field2,k_r,-1.5,pixel_size,wavelength_r,0.1)\n",
    "phase_predict = np.angle(complex_holo_predict)\n",
    "phase_predict = np.abs(phase_predict)/np.max(np.abs(phase_predict))\n",
    "# phase_predict = phase_predict*np.pi\n",
    "plt.imshow(phase_predict,cmap='gray')\n",
    "print(phase_predict.shape)\n",
    "# plt.imsave('./image_data/slm_phase 실험해볼 것/2D_phase_band_limited_r/U2_11_2d_img_e20_b8_lr4d_f06_band_angular_2_Bird_f15_bl_phase.png',phase_predict,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주파수 성분 확인(위상 홀로그램을 푸리에 변환하면 볼 수 있음)\n",
    "# phase1 = np.exp(1j*phase)\n",
    "phase_frequency = np.fft.fft2(phase)\n",
    "phase_frequency = np.fft.fftshift(phase_frequency)\n",
    "# phase_frequency1 = np.abs(phase_frequency)/np.max(np.abs(phase_frequency))\n",
    "magnitude_spectrum = 20*np.log(np.abs(phase_frequency))\n",
    "\n",
    "plt.imshow(magnitude_spectrum,cmap='gray')\n",
    "print(magnitude_spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주파수 성분 확인(DPAC을 통해 만든 위상 홀로그램의 주파수는? 진폭 정보를 버리지 않은 상태)\n",
    "# phase1 = np.exp(1j*phase)\n",
    "DPAC_phase = double_phase_coding(complex_holo1)\n",
    "phase_frequency2 = np.fft.fft2(DPAC_phase)\n",
    "phase_frequency2 = np.fft.fftshift(phase_frequency2)\n",
    "# phase_frequency1 = np.abs(phase_frequency)/np.max(np.abs(phase_frequency))\n",
    "magnitude_spectrum2 = 20*np.log(np.abs(phase_frequency2))\n",
    "\n",
    "plt.imshow(magnitude_spectrum2,cmap='gray')\n",
    "print(magnitude_spectrum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = phase - np.mean(phase)\n",
    "recon = band_limited_angular_spectrum2(phase,k_r,1.5,pixel_size,wavelength_r,0.1)\n",
    "plt.imshow(recon,cmap='gray')\n",
    "print(recon.shape)\n",
    "# plt.imsave('./image_data/Alpha/recon/bird/general_f15_Bird_multiply.png',recon,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_predict = phase_predict - np.mean(phase_predict)\n",
    "\n",
    "recon_predict = band_limited_angular_spectrum2(phase_predict,k_r,1.5,pixel_size,wavelength_r,0.1)\n",
    "plt.imshow(recon_predict,cmap='gray')\n",
    "print(recon_predict)\n",
    "print(recon_predict.shape)\n",
    "# plt.imsave('./image_data/Alpha/recon/bird/deep_general_f15_Bird_multiply.png',recon_predict,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.squeeze(test_img)\n",
    "recon = np.squeeze(recon)\n",
    "recon_predict = np.squeeze(recon_predict)\n",
    "\n",
    "print(test_img.shape)\n",
    "print(recon.shape)\n",
    "print(recon_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = tf.cast(test_img,tf.float32)\n",
    "recon = tf.cast(recon,tf.float32)\n",
    "recon_predict = tf.cast(recon_predict,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PSNR(test_img,recon),PSNR(test_img,recon_predict))\n",
    "\n",
    "# # DPAC\n",
    "# print(PSNR(test_img,recon2),PSNR(test_img,recon_predict2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.expand_dims(test_img,axis=-1)\n",
    "recon = np.expand_dims(recon,axis=-1)\n",
    "recon_predict = np.expand_dims(recon_predict,axis=-1)\n",
    "# recon2 = np.expand_dims(recon2,axis=-1)\n",
    "# recon_predict2 = np.expand_dims(recon_predict2,axis=-1)\n",
    "\n",
    "SSIM = tf.image.ssim(test_img,recon,max_val=1.0,filter_size=11,filter_sigma=1.5,k1=0.01,k2=0.03)\n",
    "print(SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM = tf.image.ssim(test_img,recon_predict,max_val=1.0,filter_size=11,filter_sigma=1.5,k1=0.01,k2=0.03)\n",
    "print(SSIM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu_v2.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeae8ebf71bc96da9fb4e93baa1bd01f77dfc38349975b209bb7a164152e69c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
